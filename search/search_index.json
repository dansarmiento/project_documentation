{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Analytics Portfolio Repository","text":""},{"location":"#daniel-sarmiento-analytics-engineer","title":"Daniel Sarmiento | Analytics Engineer","text":"<p>Chino, CA | (909) 973-8783 | dansarmiento@gmail.com | LinkedIn | GitHub</p>"},{"location":"#about-me","title":"About Me","text":"<p>I am an Analytics Engineer with extensive experience in the healthcare sector, I specialize in designing and deploying scalable, cloud-based data solutions in multi-cloud environments. I lead the integration of complex clinical, financial, and operational data from various EMR and enterprise systems to support real-time business intelligence and drive strategic decision-making. My technical expertise lies in architecting robust data pipelines using modern programming languages and orchestration tools , developing advanced analytical and predictive models to monitor key health and financial outcomes , and creating intuitive self-service dashboards that translate complex findings into clear, actionable insights for stakeholders. I am passionate about fostering a data-driven culture by mentoring analysts and leading training on analytics engineering best practices. This drive for innovation is reflected in my continuous professional development, where I actively pursue new skills and certifications in emerging data technologies and methodologies.</p>"},{"location":"#skills","title":"Skills","text":"<ul> <li>Cloud Data Platforms: AWS Solutions Architect, Google Cloud Platform, Microsoft Fabric</li> <li>Databases: SQL Server, Postgres, MySQL, Google BigQuery, AWS Redshift </li> <li>ETL/ELT: dbt, SQL, Python, Apache Airflow, Apache Kafka </li> <li>Programming Languages: Python, SQL </li> <li>Data Visualization: Tableau, Power BI, Google Looker </li> <li>Machine Learning: Supervised and unsupervised model training with Python </li> <li>Version Control: Github </li> <li>Epic Ecosystem: Clarity, Caboodle, and Cogito Data Models </li> </ul>"},{"location":"#portfolio-projects","title":"Portfolio Projects","text":"<p>A curated selection of projects demonstrating my skills in data engineering, predictive analytics, and business intelligence. All projects use publicly available, de-identified data.</p>"},{"location":"#project-1-building-a-scalable-e-commerce-data-warehouse-with-dbt-and-bigquery","title":"Project 1: Building a Scalable E-Commerce Data Warehouse with dbt and BigQuery","text":"<ul> <li>Description: This project demonstrates a complete, production-style analytics engineering workflow to build a data warehouse for a model e-commerce business (\"Jaffle Shop\"). Starting with raw data extracted from Google Sheets, the project involves loading the data into Google BigQuery and using the Data Build Tool (dbt) to perform robust, version-controlled transformations. The project showcases the evolution from a simple, monolithic SQL model to a scalable, multi-layered architecture, including staging models for data cleaning, intermediate models for complex logic, and final mart tables optimized for business intelligence and analysis. The final output is a clean, reliable, and well-documented data warehouse ready for analytics.</li> <li> <ul> <li>Skills Showcased: Analytics Engineering, Data Modeling (Staging, Intermediate, Marts), Data Transformation (ELT), dbt (Data Build Tool), Google BigQuery, SQL, Data Warehousing, Version Control Principles.</li> </ul> </li> <li>**Link to Repository **</li> </ul>"},{"location":"#project-2-evaluating-and-operationalizing-an-appointment-no-show-prediction-model","title":"Project 2: Evaluating and Operationalizing an Appointment No-Show Prediction Model","text":"<ul> <li>Description: This project centers on the comprehensive evaluation of a machine learning model designed to predict patient no-shows for medical appointments using data from an EPIC EHR system. The analysis includes retrieving and preparing appointment data, followed by a multi-faceted assessment of the model's predictive probabilities. Key evaluation steps involve examining the distribution of predictions across different appointment statuses, calculating standard classification metrics (ROC AUC, Precision, Recall, F1-score), and performing statistical tests (ANOVA, Tukey HSD) to confirm the model's ability to differentiate between outcome groups. The project also delves into model calibration by analyzing outcome frequencies within prediction percentile bins and segments performance by clinical specialty. The final output provides a clear understanding of the model's strengths and limitations, offering actionable recommendations for deploying tiered intervention strategies and dashboards to reduce no-show rates and enhance operational efficiency.</li> <li>Skills Showcased: Healthcare Analytics, Machine Learning Model Evaluation, Statistical Analysis, Data Visualization, Data Retrieval (SQLAlchemy, SQL)</li> <li>Link to Repository</li> </ul>"},{"location":"#project-3-local-pdf-question-answering-with-retrieval-augmented-generation-rag","title":"Project 3: Local PDF Question-Answering with Retrieval-Augmented Generation (RAG)","text":"<ul> <li>Description: Developed a complete, end-to-end question-answering system in a Python notebook that leverages local Large Language Models (LLMs) to answer questions about user-uploaded PDF documents. The project ensures total data privacy by installing and running all components\u2014including the Ollama service, the llama3.2 language model, and the nomic-embed-text embedding model\u2014directly within a self-contained environment. The pipeline dynamically ingests and processes any number of PDFs, splits them into manageable text chunks, and generates vector embeddings to create a searchable ChromaDB knowledge base. Using the LangChain framework, it constructs a sophisticated RAG chain that retrieves the most relevant document context to answer user questions, ensuring the LLM's responses are grounded in factual, source-based information and preventing data from ever leaving the local machine.</li> <li>Skills Showcased: Python, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), LangChain, Ollama, ChromaDB, Vector Embeddings, Natural Language Processing (NLP), Data Ingestion, Document Processing</li> <li>Link to Repository</li> </ul>"},{"location":"#project-4-machine-learning-patient-appointment-feature-store-design","title":"Project 4: Machine Learning Patient Appointment Feature Store Design","text":"<ul> <li>Description: Designed and implemented a prototype feature store using Python and SQLite to manage, version, and evaluate features derived from patient appointment data. This project involved defining a comprehensive metadata model for feature definitions, transformation logic, feature sets, lineage, and evaluation metrics. The workflow included ingesting raw appointment data, performing feature engineering (e.g., extracting time-based features, creating categorical and boolean flags), registering these features, storing them, and conducting illustrative evaluations (Chi-Squared tests) against a target variable (\"Left without seen\"). The system also included visualization of feature distributions and evaluation results to aid in feature understanding and selection for potential predictive modeling.</li> <li>Skills Showcased: Python, Pandas, SQLite, Data Modeling, Feature Engineering, Machine Learning Feature Evaluation, Data Visualization, Metadata Management</li> <li>Link to Repository</li> </ul>"},{"location":"ColaboratoryNotebooks/","title":"ColaboratoryNotebooks Project Documentation","text":""},{"location":"ColaboratoryNotebooks/#overview","title":"Overview","text":"<p>This project, <code>ColaboratoryNotebooks</code>, contains a collection of Jupyter Notebooks demonstrating various data analysis techniques using Google Colaboratory.  The notebooks cover a range of topics, from basic statistical analysis and feature engineering to more advanced machine learning models, showcasing practical applications of data analysis within the Google Colab environment.</p>"},{"location":"ColaboratoryNotebooks/#key-features","title":"Key Features","text":"<ul> <li>Variety of Data Analysis Techniques:  The notebooks demonstrate a wide array of techniques, including linear regression (simple and multiple), logistic regression, Naive Bayes, ARIMA modeling, time series analysis with FBProphet, and deep learning using Keras and TensorFlow.</li> <li>Real-world Dataset Applications:  Multiple notebooks utilize real-world datasets for practical demonstrations, covering diverse areas like healthcare (CMS data), sports (water polo statistics), and potentially others indicated by the file names.</li> <li>Comprehensive Feature Engineering Examples:  Several notebooks are dedicated to feature engineering, a crucial aspect of effective data analysis.</li> <li>Data Visualization:  Notebooks utilize libraries like Plotly for creating interactive visualizations.</li> <li>Google Colaboratory Integration:  All notebooks are designed to run seamlessly within Google Colaboratory.</li> </ul>"},{"location":"ColaboratoryNotebooks/#project-structure","title":"Project Structure","text":"<p>The project is primarily organized with individual Jupyter Notebook files (.ipynb). The notebooks are directly in the root directory, with a subset also contained within the <code>notebooks</code> directory.  Key files and directories include:</p> <ul> <li><code>/</code> (Root Directory): Contains the majority of Jupyter Notebooks demonstrating various data analysis techniques.</li> <li><code>notebooks/</code>: A subdirectory containing additional Jupyter Notebooks.</li> <li><code>README.md</code>: This documentation file.</li> <li><code>LICENSE</code>:  The project license.</li> </ul>"},{"location":"ColaboratoryNotebooks/#getting-started","title":"Getting Started","text":"<p>To use these notebooks, follow these steps:</p> <ol> <li>Access Google Colaboratory: Go to colab.research.google.com.</li> <li>Create a New Notebook: Click \"New Notebook\" to create a new, blank Jupyter Notebook.</li> <li>Upload Notebooks:  Upload the desired <code>.ipynb</code> file(s) from this repository into your Colab environment. You can do this by clicking \"Files\" on the left sidebar and then \"Upload\".</li> <li>Run the Notebooks: Open the uploaded notebook and run each code cell sequentially using the \"Play\" button or Shift+Enter keyboard shortcut.  Make sure to install any necessary libraries as indicated within the notebook itself (e.g., <code>pandas</code>, <code>scikit-learn</code>, <code>tensorflow</code>, <code>plotly</code>).</li> </ol> <p>Example (Illustrative):  Most notebooks will begin by importing necessary libraries and loading data.  A typical cell might look like this:</p> <pre><code>import pandas as pd\nimport numpy as np\n# ... other imports ...\n\ndata = pd.read_csv(\"data.csv\") # Replace \"data.csv\" with the actual filename\n# ... rest of the notebook code ...\n</code></pre> <p>Remember that each notebook is self-contained and may require specific data files or libraries to be installed and imported. Consult the individual notebooks for detailed instructions and dependencies.</p>"},{"location":"analytics_portfolio/","title":"<code>analytics_portfolio</code> Project Documentation","text":"<p>This repository showcases a collection of data analysis projects demonstrating expertise in full-stack analytics, encompassing ETL processes, statistical analysis, data visualization, and machine learning.  The projects utilize publicly available, de-identified data and highlight a range of skills applicable to diverse analytical challenges.</p>"},{"location":"analytics_portfolio/#key-features","title":"Key Features","text":"<ul> <li>End-to-End Analytics: Projects cover the entire analytics lifecycle, from data extraction and transformation to model building and visualization.</li> <li>Diverse Technologies:  Utilizes a variety of tools and technologies including SQL, Python, dbt, BigQuery, various cloud platforms, and machine learning libraries.</li> <li>Healthcare Focus: Several projects specifically address challenges within the healthcare domain, leveraging data from Electronic Health Records (EHR).</li> <li>Well-Documented Projects: Each project includes a detailed description, highlighting the methodology, skills used, and results achieved.</li> <li>Reproducible Workflows: Notebooks are provided to allow for reproducibility and exploration of the analytical processes.</li> </ul>"},{"location":"analytics_portfolio/#project-structure","title":"Project Structure","text":"<p>The repository contains Jupyter Notebooks (.ipynb) detailing individual projects and supporting documentation.</p> <ul> <li><code>.ipynb</code> files: These contain the code and analysis for each project.  Specific project titles are descriptive of their contents.</li> <li><code>README.md</code>: This file provides an overview of the repository and its contents.</li> <li><code>LICENSE</code>: The license under which the code is distributed.</li> <li><code>daniel sarmiento - Epic Certificate Status.pdf</code>:  Likely a supporting document unrelated to the core projects.</li> </ul>"},{"location":"analytics_portfolio/#getting-started","title":"Getting Started","text":"<p>Each project within this repository is self-contained and can be run independently.  The following steps provide a general guide to working with the Jupyter Notebooks:</p> <ol> <li> <p>Clone the Repository: Clone this repository to your local machine using Git:  <code>git clone https://github.com/dansarmiento/analytics_portfolio.git</code></p> </li> <li> <p>Install Dependencies:  Each notebook has specific dependencies. Install them as needed based on the project's requirements and instructions within the notebook itself.  The README provides a general overview of technologies and skills employed across projects (e.g., Python, SQL, dbt, BigQuery, etc.).</p> </li> <li> <p>Run the Notebooks: Open the desired <code>.ipynb</code> file in a Jupyter Notebook environment.  Follow the instructions within the notebook to execute the code and reproduce the analysis.  Note that some projects (especially those involving cloud services like BigQuery) may require specific configurations and credentials.</p> </li> <li> <p>Data Access: Note that the projects often leverage publicly available datasets; however, accessing or replicating certain data sources (like EHR data) might require additional steps and/or appropriate authorization.</p> </li> </ol> <p>Example (Project 1): To run the \"Building a Scalable E-Commerce Data Warehouse\" project, open <code>dbt_bigquery_in_colab.ipynb</code>.  You will need to ensure you have a Google Cloud Project configured and have installed the necessary libraries (including the <code>dbt</code> package and the appropriate BigQuery client libraries). The notebook will guide you through the steps.</p> <p>This documentation provides a high-level overview.  For detailed instructions and specific requirements, refer to the individual project notebooks and their accompanying descriptions within the <code>README.md</code> file.</p>"},{"location":"clarity_omop/","title":"clarity_omop: OMOP Database Creation from Clarity Data","text":""},{"location":"clarity_omop/#overview","title":"Overview","text":"<p>This project provides SQL scripts and ETL processes for constructing an Observational Medical Outcomes Partnership (OMOP) compliant database from a Clarity data source.  The scripts handle the extraction, transformation, and loading (ETL) of data, meticulously following the specified insertion and deletion order to maintain data integrity within the complex OMOP schema.  The README details crucial dependencies between OMOP tables, emphasizing the importance of a sequential approach for both data insertion and deletion to avoid inconsistencies.</p>"},{"location":"clarity_omop/#key-features","title":"Key Features","text":"<ul> <li>Comprehensive ETL Processes:  A complete set of SQL scripts for extracting and transforming data from a Clarity source into an OMOP CDM.</li> <li>Strict Order of Operations:  Scripts adhere to a defined insertion and deletion order to maintain referential integrity within the OMOP database.</li> <li>Modular Design: The SQL scripts are organized into individual files, allowing for flexibility and maintainability.</li> <li>Support for Multiple Data Sources: The scripts appear to handle various data types including ambulatory (AMB), inpatient (HSP), and anesthesiology (ANES) data.</li> <li>OMOP CDM Compliance: The database structure and data transformation procedures aim for compliance with the OMOP Common Data Model.</li> </ul>"},{"location":"clarity_omop/#project-structure","title":"Project Structure","text":"<p>The project is structured primarily with SQL files organized by OMOP table and data source.  Key directories and files include:</p> <ul> <li>SQL Scripts:  Numerous <code>.sql</code> files containing the ETL processes for various OMOP tables (e.g., <code>person.sql</code>, <code>visit_occurrence.sql</code>, <code>condition_occurrence.sql</code>, etc.).  These are further categorized by data source (e.g., AMB, HSP, ANES) and data type (e.g.,  ICD, SNOMED, CPT).  <code>pull_*.sql</code> files likely represent the extraction step, while <code>app_*.sql</code> files appear to handle data application and transformation.</li> <li>README.md: This file provides critical information on the insertion and deletion order for the OMOP tables.</li> </ul>"},{"location":"clarity_omop/#getting-started","title":"Getting Started","text":"<p>This project requires a database system (e.g., PostgreSQL, MySQL) capable of executing SQL scripts.  The exact steps for running these scripts will depend on your chosen database and the Clarity data source connection details.  However, a general workflow would be:</p> <ol> <li>Database Setup: Create a new database and user with appropriate permissions.</li> <li>Data Source Connection: Configure access to your Clarity data source. This typically involves defining connection strings or environment variables.</li> <li>Sequential Execution: The SQL scripts must be executed in the order specified by the <code>README.md</code>.  Begin with the tables at the beginning of the insertion order (<code>location.sql</code>, <code>care_site.sql</code>, etc.) and proceed sequentially to the tables at the end of the insertion order.</li> <li>Error Handling: Implement robust error handling mechanisms to manage potential issues during script execution.</li> </ol> <p>Example (Illustrative): Assuming you're using <code>psql</code> with a Postgres database:</p> <pre><code>psql -d my_omop_database -U my_user -f location.sql\npsql -d my_omop_database -U my_user -f care_site.sql\n...and so on, following the Insertion Order detailed in the README.\n</code></pre> <p>Remember to replace <code>my_omop_database</code> and <code>my_user</code> with your database name and user credentials, respectively.  Consult your database system's documentation for specific instructions on running SQL scripts.  Thorough testing is crucial after database population.  The deletion order, also specified in the README, will be necessary for database updates or complete data refreshes.</p>"},{"location":"database_engineering_mysql/","title":"Database Engineering Project: Little Lemon Restaurant","text":""},{"location":"database_engineering_mysql/#overview","title":"Overview","text":"<p>This project, <code>database_engineering_mysql</code>, focuses on the design and implementation of a relational database for the Little Lemon Restaurant.  It utilizes MySQL and includes various SQL scripts for database schema creation, data manipulation, stored procedures, and view creation. The project demonstrates practical database engineering skills, including forward engineering, data modeling, and the use of advanced SQL features.</p>"},{"location":"database_engineering_mysql/#key-features","title":"Key Features","text":"<ul> <li>Database Schema Design: A complete relational database schema for managing restaurant data (likely bookings, orders, etc.).</li> <li>SQL Scripts:  Multiple SQL files implementing database creation, stored procedures, views, and other SQL operations.</li> <li>Data Modeling Diagram: A visual representation of the database schema (<code>little_lemon_sales_data_model.png</code>).</li> <li>Stored Procedures:  A suite of stored procedures for managing restaurant bookings (add, update, cancel, check).</li> <li>Views and Queries: SQL queries demonstrating data retrieval using joins and subqueries.</li> <li>Prepared Statements: Example of using prepared statements for efficient and secure query execution.</li> </ul>"},{"location":"database_engineering_mysql/#project-structure","title":"Project Structure","text":"<p>The project directory contains the following key files and folders:</p> <ul> <li><code>LittleLemonDB_forward_engineer.sql</code>:  SQL script for creating the initial database schema.</li> <li><code>db_capstone_project.ipynb</code>:  Likely a Jupyter Notebook containing analysis or experimentation related to the database.</li> <li><code>little_lemon_sales_data_model.png</code>:  Diagram visualizing the database schema.</li> <li><code>little_lemon_sales_model.mwb</code>:  Likely a database model file (MySQL Workbench).</li> <li><code>m2_*.sql</code>:  Series of SQL files demonstrating specific database operations (stored procedures, views, queries, etc.).  These files are likely modular components for different aspects of the database management.</li> <li><code>README.md</code>: This file.</li> </ul>"},{"location":"database_engineering_mysql/#getting-started","title":"Getting Started","text":"<p>This project requires a MySQL server to be installed and running.  The following steps outline how to use the provided SQL scripts:</p> <ol> <li>Install MySQL: Ensure you have MySQL Server installed and configured on your system.</li> <li>Create a MySQL database: Create a new database (e.g., <code>LittleLemonDB</code>).</li> <li>Execute the schema script: Connect to the newly created database and execute <code>LittleLemonDB_forward_engineer.sql</code> to create the tables.  This can be done using the MySQL command-line client or a GUI tool like MySQL Workbench.</li> <li>Execute other SQL scripts:  Sequentially execute the other <code>.sql</code> files (e.g., those starting with <code>m2_</code>) to create stored procedures, views, and populate the database with sample data (if any is provided).  The order might be important; consult the file names for logical execution order.</li> <li>Interact with the database: Use the MySQL client or a preferred tool to query the database using the created views and stored procedures.</li> </ol> <p>Example (using the MySQL command-line client):</p> <pre><code>mysql -u your_username -p LittleLemonDB &lt; LittleLemonDB_forward_engineer.sql\nmysql -u your_username -p LittleLemonDB &lt; m2_create_stored_procedure.sql\n# ... execute other .sql files ...\n</code></pre> <p>Remember to replace <code>your_username</code> with your MySQL username.  The Jupyter Notebook (<code>db_capstone_project.ipynb</code>) may provide further instructions or examples.  Consult the file <code>little_lemon_sales_data_model.png</code> for understanding the database schema.</p>"},{"location":"dbt_data_build_tool/","title":"dbt_data_build_tool: Data Modeling with dbt","text":""},{"location":"dbt_data_build_tool/#overview","title":"Overview","text":"<p>This project demonstrates the benefits of using dbt (data build tool) for building and managing data models.  It translates a traditional SQL view (<code>vw_patient_profile_consolidated</code>) into a series of modular dbt models, improving maintainability, scalability, and data governance. This approach leverages dbt's features for modularity, testing, version control, and automated documentation generation, resulting in a more robust and manageable analytics pipeline.</p>"},{"location":"dbt_data_build_tool/#key-features","title":"Key Features","text":"<ul> <li>Modular Data Modeling:  Breaks down complex transformations into smaller, reusable dbt models.</li> <li>Automated Testing: Implements data quality checks using dbt's testing framework.</li> <li>Version Control: Manages models as <code>.sql</code> files within Git for collaboration and tracking changes.</li> <li>Automated Documentation: Generates a searchable data dictionary using <code>dbt docs generate</code>.</li> <li>Environment Management: Supports deployment across multiple environments (Dev, Staging, Prod).</li> <li>Performance Optimization: Allows for flexible materialization strategies (view, table, incremental).</li> <li>Clear Data Lineage: Provides a visual DAG (Directed Acyclic Graph) illustrating model dependencies.</li> </ul>"},{"location":"dbt_data_build_tool/#project-structure","title":"Project Structure","text":"<p>The project is organized as follows:</p> <ul> <li><code>LICENSE</code>:  The project's open-source license.</li> <li><code>README.md</code>: This documentation file.</li> <li><code>dbt_patient_profile</code>: The core dbt project directory.<ul> <li><code>dbt_project.yml</code>: The dbt project configuration file.</li> <li><code>models</code>: Contains all dbt models, organized into subfolders:<ul> <li><code>staging</code>:  Initial data transformation models (e.g., <code>stg_visits.sql</code>, <code>stg_visit_scheduled.sql</code>).</li> <li><code>intermediate</code>: Intermediate transformation models (e.g., <code>int_demographics.sql</code>, <code>int_dept_counts.sql</code>, <code>int_visit_agg.sql</code>, <code>int_visit_total.sql</code>).</li> <li><code>marts</code>: Final, consolidated fact tables (e.g., <code>fct_patient_profile_consolidated.sql</code>).</li> </ul> </li> </ul> </li> <li><code>rpt_patient_profile.sql</code>: (Likely a legacy SQL view that the dbt models replace).</li> </ul>"},{"location":"dbt_data_build_tool/#getting-started","title":"Getting Started","text":"<p>This project requires dbt to be installed.  Follow the instructions on the dbt website for installation.  Once installed:</p> <ol> <li>Clone the repository: <code>git clone &lt;repository_url&gt;</code></li> <li>Navigate to the project directory: <code>cd dbt_data_build_tool/dbt_patient_profile</code></li> <li>Configure your dbt profile:  You'll need to create a <code>profiles.yml</code> file (see dbt documentation for details) pointing to your data warehouse.</li> <li>Run dbt:</li> <li>To compile the models: <code>dbt run</code></li> <li>To test the models: <code>dbt test</code></li> <li>To generate documentation: <code>dbt docs generate</code></li> </ol> <p>This will build the dbt models, run any associated tests, and (optionally) generate documentation based on your configured profile.  The specific SQL used for each model is defined in the <code>.sql</code> files within the <code>models</code> directory. Examine <code>fct_patient_profile_consolidated.sql</code> for an example of how the final consolidated model incorporates the intermediate models.  Remember to consult the dbt documentation for more advanced usage and configuration options.</p>"},{"location":"machine_learning_notebooks/","title":"machine_learning_notebooks","text":"<p>This project is a collection of Jupyter Notebooks demonstrating various machine learning techniques and applications.  The notebooks cover a range of topics, from fundamental algorithms to advanced model building and deployment using Spark ML.  This repository serves as a practical learning resource and a showcase of different approaches to common machine learning problems.</p>"},{"location":"machine_learning_notebooks/#key-features","title":"Key Features","text":"<ul> <li>Comprehensive coverage of machine learning algorithms: Includes examples of supervised learning (regression, classification), unsupervised learning (clustering, dimensionality reduction), and neural networks.</li> <li>Real-world datasets and applications: Notebooks utilize various datasets, addressing diverse problems such as heart failure prediction, mental health analysis, and weather forecasting.</li> <li>Spark ML integration: Demonstrates the application of machine learning algorithms using Apache Spark for large-scale data processing.</li> <li>Pipeline development: Examples of creating and using machine learning pipelines for improved efficiency and reproducibility.</li> <li>Model evaluation and comparison: Notebooks emphasize proper model evaluation techniques, allowing for informed model selection.</li> </ul>"},{"location":"machine_learning_notebooks/#project-structure","title":"Project Structure","text":"<p>The project is organized into folders based on the type of machine learning technique used:</p> <ul> <li><code>EDA</code>: Exploratory Data Analysis notebooks.</li> <li><code>neural_networks</code>: Notebooks focused on neural network architectures and training.</li> <li><code>pipeline</code>: Notebooks demonstrating the creation and use of machine learning pipelines.</li> <li><code>spark_ml</code>: Notebooks showcasing Spark ML library functionalities.</li> <li><code>supervised</code>: Notebooks covering various supervised learning algorithms and applications.</li> <li><code>unsupervised</code>: Notebooks covering various unsupervised learning algorithms and applications.</li> </ul> <p>Individual notebooks within these directories address specific algorithms or applications.  For example, <code>supervised/logistic_regression_for_classification.ipynb</code> demonstrates logistic regression for a classification task.</p>"},{"location":"machine_learning_notebooks/#getting-started","title":"Getting Started","text":"<p>Each Jupyter Notebook in this repository is self-contained. To run the notebooks, you will need:</p> <ol> <li>Jupyter Notebook: Install Jupyter Notebook using <code>pip install notebook</code>.</li> <li>Python Libraries:  Install the necessary Python libraries.  The specific libraries required may vary between notebooks, but common dependencies include <code>pandas</code>, <code>scikit-learn</code>, <code>matplotlib</code>, <code>seaborn</code>, and potentially <code>pyspark</code> for Spark ML notebooks.  You can find the specific requirements by examining the import statements within each notebook.</li> </ol> <p>To run a notebook:</p> <ol> <li>Clone the repository: <code>git clone https://github.com/[YourGitHubUsername]/machine_learning_notebooks.git</code> (Replace <code>[YourGitHubUsername]</code> with the actual username).</li> <li>Navigate to the directory containing the notebook you wish to run.</li> <li>Start Jupyter Notebook: <code>jupyter notebook</code>.</li> <li>Open the desired <code>.ipynb</code> file in your browser.</li> <li>Execute the cells in the notebook sequentially.</li> </ol> <p>Note: Some notebooks may require specific datasets.  Instructions for data acquisition will be provided within those notebooks if necessary.  Ensure you have the necessary data files before running those notebooks.</p>"},{"location":"python_analytics_solutions/","title":"python_analytics_solutions: Jupyter Notebook Solutions for ETL and Data Analytics","text":""},{"location":"python_analytics_solutions/#overview","title":"Overview","text":"<p>This repository contains a collection of Jupyter Notebooks designed to solve various Extract, Transform, Load (ETL) and data analytics tasks.  The notebooks utilize different technologies like Apache Airflow, SQL Alchemy, Spark ML, and handle diverse data formats and sources.  These solutions are provided as examples and may require adaptation to specific environments and data structures.</p>"},{"location":"python_analytics_solutions/#key-features","title":"Key Features","text":"<ul> <li>ETL Processes:  Multiple notebooks demonstrate ETL workflows for different data sources and formats.</li> <li>Data Analytics:  Notebooks showcase various data analysis techniques, including feature engineering and machine learning model building.</li> <li>Technology Diversity:  Leverages Python libraries such as Apache Airflow, SQL Alchemy, and Apache Spark for data processing and analysis.</li> <li>Real-world Examples: Notebooks address practical data challenges, including updates to various systems (CEMRP, MGMA, Productivity, Provider) and data transformations (XLS to XLSX).</li> <li>Machine Learning: Notebooks demonstrate the use of Spark ML for building machine learning pipelines (including Logistic Regression).</li> </ul>"},{"location":"python_analytics_solutions/#project-structure","title":"Project Structure","text":"<p>The repository is organized with each Jupyter Notebook representing a distinct solution:</p> <ul> <li><code>.ipynb</code> files: Individual Jupyter Notebooks containing ETL and data analytics code.  Examples include <code>Apache_Airflow_Python.ipynb</code>, <code>Spark_ML_Pipeline.ipynb</code>, and others focused on specific data processing tasks.</li> <li><code>apache_spark_hdfs</code> directory: (Presumed) This directory likely contains supporting files or data for notebooks utilizing Apache Spark and HDFS.  Further investigation is needed to confirm its contents.</li> <li><code>LICENSE</code> file: Contains the project's licensing information.</li> <li><code>README.md</code> file: This file (the current document).</li> </ul>"},{"location":"python_analytics_solutions/#getting-started","title":"Getting Started","text":"<p>To use these notebooks, you will need:</p> <ol> <li>Jupyter Notebook: Install Jupyter Notebook using <code>pip install notebook</code>.</li> <li>Python Libraries: Install necessary Python libraries based on the notebook's requirements. This will likely include <code>pandas</code>, <code>psycopg2</code> (or other database connectors), and potentially <code>pyspark</code> and <code>apache-airflow</code> (or their respective alternatives). Specific library requirements are not explicitly stated in the provided information and will need to be determined by inspecting each individual notebook's code.</li> <li>Data: You will need to provide the relevant data sources as specified within each individual Jupyter Notebook.</li> </ol> <p>Running a Notebook:</p> <ol> <li>Clone the repository: <code>git clone https://github.com/&lt;username&gt;/python_analytics_solutions.git</code> (Replace <code>&lt;username&gt;</code> with the actual repository username).</li> <li>Navigate to the cloned directory.</li> <li>Launch Jupyter Notebook: <code>jupyter notebook</code>.</li> <li>Open the desired <code>.ipynb</code> file from the Jupyter Notebook interface.</li> <li>Execute the code cells sequentially, adapting the code and data paths as needed.</li> </ol> <p>Example (Conceptual): To run <code>Spark_ML_Pipeline.ipynb</code>, you will likely need to configure Spark, load your data into a Spark DataFrame, and then execute the code within the notebook.  The exact steps depend on the specific code within the notebook itself.</p> <p>Note:  This documentation assumes a basic understanding of Jupyter Notebooks, Python, and the technologies used within the various notebooks.  Detailed instructions for each notebook are best found within the individual notebook files themselves.  The <code>apache_spark_hdfs</code> directory's purpose requires further investigation.</p>"},{"location":"sql_solutions/","title":"sql_solutions: Frequently Requested Reports","text":""},{"location":"sql_solutions/#overview","title":"Overview","text":"<p>This repository serves as a central location for frequently requested SQL queries and stored procedures.  It aims to improve efficiency and consistency by providing readily available, well-tested solutions for common reporting needs.  This eliminates the need to repeatedly write and debug similar queries, saving developers time and reducing errors.</p>"},{"location":"sql_solutions/#key-features","title":"Key Features","text":"<ul> <li>Pre-built SQL Queries:  A collection of optimized SQL queries for various reporting requirements.</li> <li>Stored Procedures: Includes stored procedures for more complex operations, offering enhanced performance and reusability.</li> <li>Views:  Provides pre-defined views to simplify data access and reporting.</li> <li>Business Logic Functions:  Contains functions encapsulating business logic, ensuring consistent calculations across different queries.  (e.g., <code>fn_business_days_calculation.sql</code>)</li> </ul>"},{"location":"sql_solutions/#project-structure","title":"Project Structure","text":"<p>The repository is structured simply, with all SQL files stored directly in the root directory:</p> <ul> <li><code>LICENSE</code>: The project's license file.</li> <li><code>README.md</code>: This documentation file.</li> <li><code>*.sql</code>:  Individual SQL files containing queries, stored procedures, functions, and views.  File names are descriptive and indicate their purpose (e.g., <code>vw_appt_kpi_current.sql</code> is a view related to appointment KPIs).</li> </ul>"},{"location":"sql_solutions/#getting-started","title":"Getting Started","text":"<p>To use the SQL queries and stored procedures in this repository, you will need to connect to the appropriate database using a SQL client (e.g., SQL Server Management Studio, DBeaver, pgAdmin).  The specific instructions for connecting will depend on your database system and client.</p> <p>Once connected, you can execute the SQL files directly. For example, to execute the <code>vw_appt_kpi_current.sql</code> view, open the file in your SQL client and execute the SQL code contained within.  The resulting view will then be available for querying.</p> <p>Remember to replace any placeholder values (e.g., database names, table names) with your specific environment's details before executing the SQL code.  Consult the individual file names and contents for details on their purpose and required parameters.</p> <p>Example (using SQL Server Management Studio):</p> <ol> <li>Open SQL Server Management Studio.</li> <li>Connect to your database server.</li> <li>Open the <code>vw_appt_kpi_current.sql</code> file.</li> <li>Execute the SQL code within the file.</li> <li>Query the newly created view using a <code>SELECT</code> statement:  <code>SELECT * FROM vw_appt_kpi_current;</code></li> </ol> <p>This repository provides a starting point for building upon these reports or creating similar ones.  Remember to always test thoroughly before using these queries in a production environment.</p>"}]}